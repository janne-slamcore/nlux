!function(e,t){"object"==typeof exports&&"undefined"!=typeof module?t(exports,require("@huggingface/inference")):"function"==typeof define&&define.amd?define(["exports","@huggingface/inference"],t):t((e="undefined"!=typeof globalThis?globalThis:e||self)["@nlux/hf"]={},e.inference)}(this,function(e,t){"use strict";var n=Object.defineProperty,s=(e,t,s)=>((e,t,s)=>t in e?n(e,t,{enumerable:!0,configurable:!0,writable:!0,value:s}):e[t]=s)(e,"symbol"!=typeof t?t+"":t,s);class o extends Error{constructor(e={}){super(e.message),s(this,"exceptionId"),s(this,"message"),s(this,"source"),s(this,"type"),this.message=e.message??"",this.source=e.source,this.type=this.constructor.name,this.exceptionId=e.exceptionId}}class r extends o{}class i extends o{}const a=e=>{"string"!=typeof e?e&&"function"==typeof e.toString?console.warn(`[nlux] ${e.toString()}`):console.warn("[nlux]"):console.warn(`[nlux] ${e}`)},h=e=>{if("object"==typeof e&&null!==e){const t=e;if("invalid_api_key"===t.code)return"invalid-api-key";if(t.message&&"string"==typeof t.message&&t.message.toLowerCase().includes("connection error"))return"connection-error"}return null};var c=Object.defineProperty,u=(e,t,n)=>((e,t,n)=>t in e?c(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n)(e,"symbol"!=typeof t?t+"":t,n);const d=class e{constructor(e){if(u(this,"__instanceId"),u(this,"inference"),u(this,"options"),!e.model&&!e.endpoint)throw new i({source:this.constructor.name,message:'when creating the Hugging Face adapter, you must set either the model or the endpoint using the "endpoint" option!'});this.__instanceId=`${this.info.id}-${"xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx".replace(/[xy]/g,e=>{const t=16*Math.random()|0;return("x"==e?t:3&t|8).toString(16)})}`,this.options={...e},this.inference=new t.HfInference(e.authToken)}get dataTransferMode(){return this.options.dataTransferMode??e.defaultDataTransferMode}get id(){return this.__instanceId}get info(){return{id:"hugging-face-adapter",capabilities:{chat:!0,fileUpload:!1,textToSpeech:!1,speechToText:!1}}}async batchText(t){if(!this.options.model&&!this.options.endpoint)throw new i({source:this.constructor.name,message:'Unable to send message! When sending a message to the Hugging Face API, you must set either the model using the "model" option or the endpoint using the "endpoint" option!'});const n={inputs:t,parameters:{max_new_tokens:this.options.maxNewTokens??e.defaultMaxNewTokens}};try{let e;if(this.options.endpoint){const t=this.inference.endpoint(this.options.endpoint);e=await t.textGeneration(n)}else e=await this.inference.textGeneration({model:this.options.model,...n});return e}catch(e){const t=e.message||"An error occurred while sending the message to the Hugging Face API";throw new o({source:this.constructor.name,message:t,exceptionId:h(e)??void 0})}}preProcessAiBatchedMessage(e,t){throw new Error("Method not implemented.")}preProcessAiStreamedChunk(e,t){throw new Error("Method not implemented.")}streamText(t,n){Promise.resolve().then(async()=>{if(!this.options.model&&!this.options.endpoint)throw new i({source:this.constructor.name,message:'Unable to send message! When sending a message to the Hugging Face API, you must set either the model using the "model" option or the endpoint using the "endpoint" option!'});const s={inputs:await this.encode(t),parameters:{max_new_tokens:this.options.maxNewTokens??e.defaultMaxNewTokens}};let o;try{if(this.options.endpoint){o=this.inference.endpoint(this.options.endpoint).textGenerationStream(s)}else o=this.inference.textGenerationStream({model:this.options.model,...s});for(;o;){const e=await o.next(),{done:t,value:s}=e;if(t)break;n.next(await this.decode(s?.token))}n.complete()}catch(e){const t=e;n.error(t),a("An error occurred while sending the message to the Hugging Face streaming API: \n"+t.message)}})}async decode(e){const t=(()=>{if("string"==typeof e)return e;if(Array.isArray(e)){if(0===e.length)return"";const t=e[0];if("object"==typeof t&&t&&"string"==typeof t.generated_text)return t.generated_text}const t=e?e.generated_text:void 0;if("string"==typeof t)return t;const n=e&&"object"==typeof e&&"text"in e?e.text:void 0;return"string"===n?n:""})(),{preProcessors:{output:n}={}}=this.options;return n?Promise.resolve(n(t)):Promise.resolve(t)}async encode(e){const t=e,{preProcessors:{input:n}={}}=this.options;if(n&&t){if("string"==typeof t)return n(t,this.options);a("The input pre-processor function was provided, but the message is not a string! Input pre-processor will not be applied.")}return e}};u(d,"defaultDataTransferMode","batch"),u(d,"defaultMaxNewTokens",500);let l=d;var p=Object.defineProperty,m=(e,t,n)=>((e,t,n)=>t in e?p(e,t,{enumerable:!0,configurable:!0,writable:!0,value:n}):e[t]=n)(e,"symbol"!=typeof t?t+"":t,n);class g{constructor(){m(this,"theAuthToken",null),m(this,"theDataTransferMode","stream"),m(this,"theEndpoint",null),m(this,"theInputPreProcessor",null),m(this,"theMaxNewTokens",null),m(this,"theModel",null),m(this,"theOutputPreProcessor",null),m(this,"theSystemMessage",null),m(this,"withDataTransferModeCalled",!1)}create(){if(!this.theModel&&!this.theEndpoint)throw new i({source:this.constructor.name,message:'You must provide a model or an endpoint using the "withModel()" method or the "withEndpoint()" method!'});return new l({dataTransferMode:this.theDataTransferMode,model:this.theModel??void 0,endpoint:this.theEndpoint??void 0,authToken:this.theAuthToken??void 0,preProcessors:{input:this.theInputPreProcessor??void 0,output:this.theOutputPreProcessor??void 0},maxNewTokens:this.theMaxNewTokens??void 0,systemMessage:this.theSystemMessage??void 0})}withAuthToken(e){if(null!==this.theAuthToken)throw new r({source:this.constructor.name,message:"Cannot set the auth token more than once"});return this.theAuthToken=e,this}withDataTransferMode(e){if(this.withDataTransferModeCalled)throw new r({source:this.constructor.name,message:"Cannot set the data loading mode more than once"});return this.theDataTransferMode=e,this.withDataTransferModeCalled=!0,this}withEndpoint(e){if(null!==this.theEndpoint)throw new r({source:this.constructor.name,message:"Cannot set the endpoint because a model or an endpoint has already been set"});return this.theEndpoint=e,this}withInputPreProcessor(e){if(null!==this.theInputPreProcessor)throw new r({source:this.constructor.name,message:"Cannot set the input pre-processor more than once"});return this.theInputPreProcessor=e,this}withMaxNewTokens(e){if(null!==this.theMaxNewTokens)throw new r({source:this.constructor.name,message:"Cannot set the max new tokens more than once"});return this.theMaxNewTokens=e,this}withModel(e){if(null!==this.theModel)throw new r({source:this.constructor.name,message:"Cannot set the model because a model or an endpoint has already been set"});return this.theModel=e,this}withOutputPreProcessor(e){if(null!==this.theOutputPreProcessor)throw new r({source:this.constructor.name,message:"Cannot set the output pre-processor more than once"});return this.theOutputPreProcessor=e,this}withSystemMessage(e){if(null!==this.theSystemMessage)throw new r({source:this.constructor.name,message:"Cannot set the system message more than once"});return this.theSystemMessage=e,this}}e.createChatAdapter=()=>new g,e.llama2InputPreProcessor=(e,t)=>`<s> [INST] <<SYS>> ${t?.systemMessage??"You are a helpful assistant. You keep your answers short."} <</SYS>> </s><s> [INST] ${e} [/INST]`,e.llama2OutputPreProcessor=e=>e?e.replace(/<[^>]*>/g,""):""});
